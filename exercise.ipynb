{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7d6e09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Build Your Own U-Net\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "In this notebook, we will implement a U-Net architecture. Through this exercise you should gain an understanding of the U-Net architecture in particular as well as learn how to approach the implementation of an architecture in general and familiarize yourself a bit more with the inner workings of pytorch.\n",
    "\n",
    "The exercise is split into three parts:\n",
    "\n",
    "In part 1 you will implement the building blocks of the U-Net. That includes the convolutions, downsampling, upsampling and skip connections. We will go in the order of how difficult they are to implement.\n",
    "\n",
    "In part 2 you will combine the modules you've built in part 1 to implement the U-Net module.\n",
    "\n",
    "In part 3 and 4 are light on coding tasks but you will learn about two important concepts: receptive fields and translational equivariance.\n",
    "\n",
    "Finally, in part 5 you will train your first U-Net of the course! This will just be a first flavor though since you will learn much more about that in the next exercise.\n",
    "\n",
    "\n",
    "Written by Larissa Heinrich, Caroline Malin-Mayor, and Morgan Schwartz, with inspiration from William Patton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf266b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## The libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095c3471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import unet_tests\n",
    "from local import (\n",
    "    NucleiDataset,\n",
    "    apply_and_show_random_image,\n",
    "    plot_receptive_field,\n",
    "    show_random_dataset_image,\n",
    "    pad_to_size,\n",
    "    unnormalize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f197eea4",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure gpu is available. Please call a TA if this cell fails\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57555ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The Dataset\n",
    "For our segmentation exercises, we will be using a nucleus segmentation dataset from [Kaggle 2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018/data). We have downloaded the dataset during setup and we provided a pytorch Dataset called `NucleiDataset` which we will use for training later. In addition to training, we will use these images to visualize the output of the individual building blocks of the U-Net we will be implementing.\n",
    "Below, we create a dataset and then visualize a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7e9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = NucleiDataset(\"nuclei_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf5c41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_random_dataset_image(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab28dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Rerun the cell above a few times to see different images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a008cdb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## The Components of a U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38059be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) architecture has proven to outperform the other architectures in segmenting biological and medical images. It is also commonly used for other tasks that require the output to be the same resolution as the input, such as style transfer and denoising. Below is an overview figure of the U-Net architecture from the original [paper](https://arxiv.org/pdf/1505.04597.pdf). We will go through each of the components first (hint: all of them can be found in the list of PyTorch modules [here](https://pytorch.org/docs/stable/nn.html#convolution-layers)), and then fit them all together to make our very own U-Net.\n",
    "\n",
    "<img src=\"static/unet.png\" alt=\"UNet\" style=\"width: 1500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2426c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Component 1: Upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea06189",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "We will start with the Upsample module that we will use in our U-Net. The right side of the U-Net contains upsampling between the levels. There are many ways to upsample: in the original U-Net, they used a transposed convolution, but this has since fallen a bit out of fashion so we will use the PyTorch Upsample Module [torch.nn.Upsample](https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html#torch.nn.Upsample) instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290668d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pytorch Modules\n",
    "Modules are the building blocks of PyTorch models, and contain lots of magic that makes training models easy. If you aren't familiar with PyTorch modules, take a look at the official documentation [here](https://pytorch.org/docs/stable/notes/modules.html). For our purposes, it is crucial to note how Modules can have submodules defined in the `__init__` function, and how the `forward` function is defined and called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76144942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here we make fake input to illustrate the upsampling techniques\n",
    "# Pytorch expects a batch and channel dimension before the actual data,\n",
    "# So this simulates a 1D input\n",
    "sample_1d_input = torch.tensor([[[1, 2, 3, 4]]], dtype=torch.float64)\n",
    "# And this simulates a 2D input\n",
    "sample_2d_input = torch.tensor(\n",
    "    [[[[1, 2], [3, 4]]]],\n",
    "    dtype=torch.float64,\n",
    ")\n",
    "sample_2d_input.shape, sample_2d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47af28",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 1: Try out different upsampling techniques</h4>\n",
    "    <p>For our U-net, we will use the built-in PyTorch Upsample Module. Here we will practice declaring and calling an Upsample module with different parameters.</p>\n",
    "    <ol>\n",
    "        <li>Declare an instance of the pytorch Upsample module with <code style=\"color: black\">scale_factor</code> 2 and mode <code style=\"color: black\">\"nearest\"</code>.</li>\n",
    "        <li>Call the instance of Upsample on the <code style=\"color: black\">sample_2d_input</code> to see what the nearest mode does.</li>\n",
    "        <li>Vary the scale factor and mode to see what changes. Check the documentation for possible modes and required input dimensions.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f3c50",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# TASK 1.1: initialize  an upsample module\n",
    "up = ...  # YOUR CODE HERE\n",
    "\n",
    "# TASK 1.2: apply your upsample module to `sample_2d_input`\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e0857",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# TASK 1.3: vary scale factor and mode\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57916d7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here is an additional example on image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4303c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apply_and_show_random_image(up, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fe6267",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Component 2: Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b53fa71",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "Between levels of the U-Net on the left side, there is a downsample step. Traditionally, this is done with a 2x2 max pooling operation. There are other ways to downsample, for example with average pooling, but we will stick with max pooling for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df068e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_2d_input = torch.tensor(np.arange(25, dtype=np.float64).reshape((1, 1, 5, 5)))\n",
    "sample_2d_input = torch.randint(0, 10, (1, 1, 6, 6))\n",
    "sample_2d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d864f6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 2A: Try out max pooling</h4>\n",
    "        <p>Using the docs for <a href=https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html>torch.nn.MaxPool2d</a>,\n",
    "        try initializing the module and applying it to the sample input. Try varying the parameters to understand the effect of <code style=\"color: black\">kernel_size</code> and <code style=\"color: black\">stride</code>.\n",
    "        </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6216f71",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# TASK 2A: Initialize max pooling and apply to sample input\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462f95e",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 2B: Implement a Downsample Module</h4>\n",
    "    <p>This is very similar to the built in MaxPool2d, but additionally has to check if the downsample factor matches in the input size. Note that we provide the forward function for you - in future Modules, you will implement the forward yourself.</p>\n",
    "    <ol>\n",
    "        <li>Declare the submodules you want to use (in this case, <code style=\"color: black\">torch.nn.MaxPool2d</code> with the correct arguments) in the <code style=\"color: black\">__init__</code> function. In our Downsample Module, we do not want to use padding and the stride should match the input kernel size.</li>\n",
    "        <li>Write a function to check if the downsample factor is valid. If the downsample factor does not evenly divide the dimensions of the input to the layer, this function should return False.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82988e6e",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class Downsample(torch.nn.Module):\n",
    "    def __init__(self, downsample_factor: int):\n",
    "        \"\"\"Initialize a MaxPool2d module with the input downsample fator\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_factor = downsample_factor\n",
    "        # TASK 2B1: Initialize the maxpool module\n",
    "        self.down = ...  # YOUR CODE HERE\n",
    "\n",
    "    def check_valid(self, image_size: tuple[int, int]) -> bool:\n",
    "        \"\"\"Check if the downsample factor evenly divides each image dimension.\n",
    "        Returns `True` for valid image sizes and `False` for invalid image sizes.\n",
    "        Note: there are multiple ways to do this!\n",
    "        \"\"\"\n",
    "        # TASK 2B2: Check that the image_size is valid to use with the downsample factor\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.check_valid(tuple(x.size()[-2:])):\n",
    "            raise RuntimeError(\n",
    "                \"Can not downsample shape %s with factor %s\"\n",
    "                % (x.size(), self.downsample_factor)\n",
    "            )\n",
    "\n",
    "        return self.down(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f947a09",
   "metadata": {
    "tags": []
   },
   "source": [
    "We wrote some rudimentary tests for each of the torch modules you are writing. If you get an error from your code or an AssertionError from the test, you should probably have another look ath your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf573754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestDown(Downsample).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf1079c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "down = Downsample(4)\n",
    "apply_and_show_random_image(down, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b79626",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Component 3: Convolution Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ff495",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "#### Convolution\n",
    "A U-Net is a convolutional neural network, which means that the main type of operation is a convolution. Convolutions with defined kernels were covered briefly in the pre-course materials.\n",
    "\n",
    "<img src=\"./static/2D_Convolution_Animation.gif\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b63ea4",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "Shown here is a 3x3 kernel being convolved with an input array to get an output array of the same size. For each pixel of the input, the value at that same pixel of the output is computed by multiplying the kernel element-wise with the surrounding 3x3 neighborhood around the input pixel, and then summing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56656555",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "#### Padding\n",
    "\n",
    "You will notice that at the edges of the input, this animation shows greyed out values that extend past the input. This is known as padding the input. This example uses \"same\" padding, which means the values at the edges are repeated. The other option we will use in this exercise is \"valid\" padding, which essentially means no padding. In the case of valid padding, the output will be smaller than the input, as values at the edges of the output will not be computed. \"Same\" padding can introduce edge artifacts, but \"valid\" padding reduces the output size at every convolution. Note that the amount of padding (for same) and the amount of size lost (for valid) depends on the size of the kernel - a 3x3 convolution would require padding of 1, a 5x5 convolution would require a padding of 2, and so on.\n",
    "\n",
    "Additionally, there are different modes of padding that determine what strategy is used to make up the values for padding. In the animation above the mode is re-using values from the border. Even more commonly, the image is simply padded with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d550c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ReLU Activation\n",
    "The Rectified Linear Unit (ReLU) is a common activation function, which takes the max of a value and 0, shown below. It introduces a non-linearity into the neural network - without a non-linear activation function, a neural network could not learn non-linear functions.\n",
    "\n",
    "<img src=\"./static/ReLU.png\" width=\"400\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c96d9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 3: Implement a ConvBlock module</h4>\n",
    "    <p>The convolution block (ConvBlock) of a standard U-Net has two 3x3 convolutions, each of which is followed by a ReLU activation. Our implementation will handle other sizes of convolutions as well. The first convolution in the block will handle changing the input number of feature maps/channels into the output, and the second convolution will have the same number of feature maps in and out.</p>\n",
    "    <ol>\n",
    "        <li>Declare the submodules you want to use in the <code style=\"color: black\">__init__</code> function. Because you will always be calling four submodules in sequence (<a href=https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d>torch.nn.Conv2d</a>, <a href=https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU>torch.nn.ReLU</a>, Conv2d, ReLU), you can use <a href=https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html>torch.nn.Sequential</a> to hold the convolutions and ReLUs.</li>\n",
    "        <li>Call the modules in the forward function. If you used <code style=\"color: black\">torch.nn.Sequential</code> in step 1, you only need to call the Sequential module, but if not, you can call the Conv2d and ReLU Modules explicitly.</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "If you get stuck, refer back to the <a href=https://pytorch.org/docs/stable/notes/modules.html>Module</a> documentation for hints and examples of how to define a PyTorch Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c2d91",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: str = \"same\",\n",
    "    ):\n",
    "        \"\"\"A convolution block for a U-Net. Contains two convolutions, each followed by a ReLU.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels for this conv block. Depends on\n",
    "                the layer and side of the U-Net and the hyperparameters.\n",
    "            out_channels (int): The number of output channels for this conv block. Depends on\n",
    "                the layer and side of the U-Net and the hyperparameters.\n",
    "            kernel_size (int): The size of the kernel. A kernel size of N signifies an\n",
    "                NxN square kernel.\n",
    "            padding (str): The type of convolution padding to use. Either \"same\" or \"valid\".\n",
    "                Defaults to \"same\".\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if kernel_size % 2 == 0:\n",
    "            msg = \"Only allowing odd kernel sizes.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        # TASK 3.1: Initialize your modules and define layers.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        for _name, layer in self.named_modules():\n",
    "            if isinstance(layer, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TASK 3.2: Apply the modules you defined to the input x\n",
    "        ...  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f7206",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "#### Test and Visualize Output of ConvBlock\n",
    "Try rerunning the visualization a few times. What do you observe? Can you explain it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d6e7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestConvBlock(ConvBlock).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf056b",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(26)\n",
    "conv = ConvBlock(1, 2, 5, \"same\")\n",
    "apply_and_show_random_image(conv, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490408a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h4>Question: Padding</h4>\n",
    "As you saw, the convolution modules in pytorch allow you to directly use the keywords <code style=\"color: black\">\"valid\"</code> or <code style=\"color: black\">\"same\"</code> for your padding mode. How would you go about calculating the amount of padding you need based on the kernel size?\n",
    "\n",
    "If you'd like, you can test your assumption by editing the `ConvBlock` to pass your own calculated value to the `padding` keyword in the conv module and rerun the test\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef18496",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Component 4: Skip Connections and Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284c4b2",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "The skip connections between the left and right side of the U-Net are central to successfully obtaining high-resolution output. At each layer, the output of the left conv block is concatenated to the output of the upsample block on the right side from the last layer below. Since upsampling, especially with the \"nearest\" algorithm, does not actually add high resolution information, the concatenation of the left side conv block output is crucial to generate high resolution segmentations.\n",
    "\n",
    "If the convolutions in the U-Net are valid, the right side will be smaller than the left side, so the left side output must be cropped before concatenation. We provide a helper function to do this cropping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87c731",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 4: Implement a CropAndConcat module</h4>\n",
    "    <p>Below, you must implement the <code style=\"color: black\">forward</code> method, including the cropping (using the provided helper function <code style=\"color: black\">center_crop</code>) and the concatenation (using <a href=https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat>torch.cat</a>).\n",
    "</p>\n",
    "Hint: Use the <code style=\"color: black\">dim</code> keyword argument of <a href=https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat>torch.cat</a> to choose along which axis to concatenate the tensors.\n",
    "</p>\n",
    "Hint: The tensors have the layout (batch, channel, x, y)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f623f",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "def center_crop(x, target_spatial_shape):\n",
    "    \"\"\"Center-crop x to match spatial dimensions given by target_spatial_shape.\"\"\"\n",
    "\n",
    "    x_target_size = x.size()[:2] + torch.Size(target_spatial_shape)\n",
    "\n",
    "    offset = tuple((a - b) // 2 for a, b in zip(x.size(), x_target_size))\n",
    "\n",
    "    slices = tuple(slice(o, o + s) for o, s in zip(offset, x_target_size))\n",
    "\n",
    "    return x[slices]\n",
    "\n",
    "\n",
    "class CropAndConcat(torch.nn.Module):\n",
    "    def forward(self, encoder_output, upsample_output):\n",
    "        upsample_spatial_shape = upsample_output.size()[2:]\n",
    "        # TASK 4: Implement the forward function\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9493929a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestCropAndConcat(CropAndConcat).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d39019",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Component 5: Output Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401073f3",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "The final block we need to write for our U-Net is the output convolution block. The exact format of output you want depends on your task, so our U-Net must be flexible enough to handle different numbers of out channels and different final activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5da0f0",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 5: Implement an OutputConv Module</h4>\n",
    "    <ol>\n",
    "        <li>Define the convolution module in the <code style=\"color: black\">__init__</code> function. You can use a convolution with kernel size 1 to get the appropriate number of output channels. The activation submodule is provided for you.</li>\n",
    "        <li>Call the final convolution and activation modules in the <code style=\"color: black\">forward</code> function</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4788e",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class OutputConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: torch.nn.Module | None = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A module that uses a convolution with kernel size 1 to get the appropriate\n",
    "        number of output channels, and then optionally applies a final activation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of feature maps that will be input to the\n",
    "                OutputConv block.\n",
    "            out_channels (int): The number of channels that you want in the output\n",
    "            activation (str | None, optional): Accepts the name of any torch activation\n",
    "                function  (e.g., ``ReLU`` for ``torch.nn.ReLU``) or None for no final\n",
    "                activation. Defaults to None.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TASK 5.1: Define the convolution submodule\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TASK 5.2: Implement the forward function\n",
    "        # YOUR CODE HERE\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1200e315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestOutputConv(OutputConv).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d55c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_conv = OutputConv(in_channels=1, out_channels=1, activation=torch.nn.ReLU())\n",
    "apply_and_show_random_image(out_conv, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296ec127",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 1</h2>\n",
    "\n",
    "Congratulations! You have implemented most of a U-Net!\n",
    "We will go over this portion together and answer any questions soon, but feel free to start on the next section where we put it all together.\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d091e",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "## Putting the U-Net together\n",
    "\n",
    "Now we will make a U-Net class that combines all of these components as shown in the image. This image shows a U-Net of depth 4 with specific input channels, feature maps, upsampling, and final activation. Ours will be configurable with regards to depth and other features.\n",
    "\n",
    "<img src=\"static/unet.png\" alt=\"UNet\" style=\"width: 1500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28801fe3",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 6: U-Net Implementation</h4>\n",
    "    <p>Now we will implement our U-Net! We have written some of it for you - follow the steps below to fill in the missing parts.</p>\n",
    "    <ol>\n",
    "        <li>Declare a list of encoder (left) and decoder (right) ConvBlocks. Carefully consider the input and output feature maps for each ConvPass!</li>\n",
    "        <li>Declare an Upsample, Downsample, CropAndConcat, and OutputConv block.</li>\n",
    "        <li>Implement the <code style=\"color: black\">forward</code> function, applying the modules you declared above in the proper order.</li>\n",
    "        </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a063c0",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int = 1,\n",
    "        final_activation: torch.nn.Module | None = None,\n",
    "        num_fmaps: int = 64,\n",
    "        fmap_inc_factor: int = 2,\n",
    "        downsample_factor: int = 2,\n",
    "        kernel_size: int = 3,\n",
    "        padding: str = \"same\",\n",
    "        upsample_mode: str = \"nearest\",\n",
    "    ):\n",
    "        \"\"\"A U-Net for 2D input that expects tensors shaped like::\n",
    "            ``(batch, channels, height, width)``.\n",
    "        Args:\n",
    "            depth:\n",
    "                The number of levels in the U-Net. 2 is the smallest that really\n",
    "                makes sense for the U-Net architecture, as a one layer U-Net is\n",
    "                basically just 2 conv blocks.\n",
    "            in_channels:\n",
    "                The number of input channels in your dataset.\n",
    "            out_channels (optional):\n",
    "                How many output channels you want. Depends on your task. Defaults to 1.\n",
    "            final_activation (optional):\n",
    "                What activation to use in your final output block. Depends on your task.\n",
    "                Defaults to None.\n",
    "            num_fmaps (optional):\n",
    "                The number of feature maps in the first layer. Defaults to 64.\n",
    "            fmap_inc_factor (optional):\n",
    "                By how much to multiply the number of feature maps between\n",
    "                layers. Encoder layer ``l`` will have ``num_fmaps*fmap_inc_factor**l``\n",
    "                output feature maps. Defaults to 2.\n",
    "            downsample_factor (optional):\n",
    "                Factor to use for down- and up-sampling the feature maps between layers.\n",
    "                Defaults to 2.\n",
    "            kernel_size (optional):\n",
    "                Kernel size to use in convolutions on both sides of the UNet.\n",
    "                Defaults to 3.\n",
    "            padding (optional):\n",
    "                How to pad convolutions. Either 'same' or 'valid'. Defaults to \"same.\"\n",
    "            upsample_mode (optional):\n",
    "                The upsampling mode to pass to torch.nn.Upsample. Usually \"nearest\"\n",
    "                or \"bilinear.\" Defaults to \"nearest.\"\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.final_activation = final_activation\n",
    "        self.num_fmaps = num_fmaps\n",
    "        self.fmap_inc_factor = fmap_inc_factor\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.upsample_mode = upsample_mode\n",
    "\n",
    "        # left convolutional passes\n",
    "        self.left_convs = torch.nn.ModuleList()\n",
    "        # TASK 6.1A: Initialize list here\n",
    "        # Loop through each level of the encoder from top (level=0) to bottom (level=self.depth - 1)\n",
    "        for level in range(self.depth): \n",
    "            # conv = \n",
    "            # Adding conv module to the list\n",
    "            self.left_convs.append(conv)\n",
    "        # right convolutional passes\n",
    "        self.right_convs = torch.nn.ModuleList()\n",
    "        # TASK 6.1B: Initialize list here\n",
    "        # Loop through each level of the decoder from top (level=0) to one above bottom (level=self.depth - 2)\n",
    "        for level in range(self.depth - 1):\n",
    "            # Initialize conv module\n",
    "            # conv = \n",
    "            # Adding conv module to the list\n",
    "            self.right_convs.append(conv)\n",
    "        \n",
    "        # TASK 6.2: Initialize other modules here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # left side\n",
    "        # Hint - you will need the outputs of each convolutional block in the encoder for the skip connection, so you need to hold on to those output tensors\n",
    "        for i in range(self.depth - 1):\n",
    "            # TASK 6.3A: Implement encoder here\n",
    "            ...\n",
    "\n",
    "        # bottom\n",
    "        # TASK 6.3B: Implement bottom of U-Net here\n",
    "\n",
    "        # right\n",
    "        for i in range(0, self.depth - 1)[::-1]:\n",
    "            # TASK 6.3C: Implement decoder here\n",
    "            ...\n",
    "        # TASK 6.3D: Apply the final convolution and return the output\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630df80a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below we declare a very simple U-Net and then apply it to a random image. Because we have not trained the U-Net the output should look similar to the output of random convolutions. If you get errors here, go back and fix your U-Net implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec9306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestUNet(UNet).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3d114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_net = UNet(depth=2, in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf95a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apply_and_show_random_image(simple_net, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1965d43c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 2</h2>\n",
    "\n",
    "Congratulations! You have implemented a UNet architecture.\n",
    "\n",
    "Next we'll learn about receptive fields which should demistify how to choose the UNet hyperparameters a little bit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3d2bfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2459ffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Receptive Field\n",
    "\n",
    "The receptive field of an output value is the set of input values that can change the output value. The size of the receptive field is an important property of network architectures for image processing. Let's consider the receptive field size of the U-Net building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad68a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h4>Question: Receptive Field Size</h4>\n",
    "What are the receptive field sizes of the following operations?\n",
    "\n",
    "1. <code style=\"color: black\">torch.nn.Conv2d(1, 5, 3)</code>\n",
    "2. <code style=\"color: black\">torch.nn.Sequential(torch.nn.Conv2d(1, 5, 3), torch.nn.Conv2d(5,5,3))</code>\n",
    "3. <code style=\"color: black\">torch.nn.Sequential(torch.nn.Conv2d(1, 5, 3), torch.nn.Conv2d(5,5,5))</code>\n",
    "4. <code style=\"color: black\">Downsample(3)</code>\n",
    "5. <code style=\"color: black\">torch.nn.Sequential(ConvBlock(1, 5, 3), Downsample(2), ConvBlock(5,5,3)</code>\n",
    "6. <code style=\"color: black\">torch.nn.Upsample(2)</code>\n",
    "7. <code style=\"color: black\">torch.nn.Sequential(ConvBlock(1,5,3), Upsample(2), ConvBlock(5,5,3))</code>\n",
    "8. <code style=\"color: black\">torch.nn.Sequential(ConvBlock(1,5,3), Downsample(3), ConvBlock(5,5,3), Upsample(3), ConvBlock(5,5,3))</code>\n",
    "9. <code style=\"color: black\">UNet(depth=2, in_channels=1, downsample_factor=3, kernel_size=3)</code>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606e05ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h4>Task 7: Receptive Field</h4>\n",
    "    <p>The <code style=\"color: black\">plot_receptive_field</code> function visualizes the receptive field of a given U-Net - the square shows how many input pixels contribute to the output at the center pixel. Try it out with different U-Nets to get a sense of how varying the depth, kernel size, and downsample factor affect the receptive field of a U-Net.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f73c8a",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "new_net = ...  # TASK 7: declare your U-Net here\n",
    "if isinstance(new_net, UNet):\n",
    "    plot_receptive_field(new_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68421249",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 3</h2>\n",
    "\n",
    "Looking at the receptive field of your network is one of the most important aspects to consider when you choose the hyperparameters for your network.\n",
    "\n",
    "Questions to consider:\n",
    "<ol>\n",
    "    <li>Which hyperparameter of the U-Net has most effect on the receptive field?</li>\n",
    "    <li>If two sets of hyperparameters result in the same receptive field size are those networks equally good choices?</li>\n",
    "    <li>What's the relation between the receptive field size and the size of images you feed to your UNet?</li>\n",
    "    <li>Do you see a connection between padding and receptive field size?</li>\n",
    "    <li>For each hyperparameter: Can you think of scenarios in which you would consider changing this parameter? Why? </li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669501c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Translational equivariance\n",
    "\n",
    "Depending on the task you're trying to solve you may care about translational (shift) invariance or equivariance.\n",
    "\n",
    "Let's first define what these invariance and equivariance mean in mathematical notation.\n",
    "\n",
    "Let $T$ be a transformation and $F$ the function whose properties we're considering.\n",
    "\n",
    "$F$ is invariant under transformation $T$ if: $F(T(x)) = F(x)$. The output of the function remains the same whether the input was transformed or not.\n",
    "\n",
    "$F$ is equivariant under transformation $T$ if: $F(T(x)) = T(F(x))$. Applying the function on the transformed input is the same as applying the transformation on the output of the original input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62053f60",
   "metadata": {
    "tags": []
   },
   "source": [
    "If math isn't your thing hopefully this picture helps to convey the concept, now specifically for translations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400400ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"static/equivariance.png\" alt=\"Invariance and Equivariance\" style=\"width: 1500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a49f94",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h4>Question: Translational invariance and equivariance</h4>\n",
    "For what types of deep learning tasks would you want your network to be translationally invariant and equivariant, respectively? Where does the U-Net fit in?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29551cb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<h4>Question: Translational properties of U-Net building blocks</h4>\n",
    "For each of these building blocks of the U-Net: Is it translationally equivariant or invariant?\n",
    "<ol>\n",
    "    <li>ConvBlock</li>\n",
    "    <li>Downsample</li>\n",
    "    <li>Upsample</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fded492",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 4</h2>\n",
    "\n",
    "\n",
    "Questions and points to consider:\n",
    "<ol>\n",
    "    <li>How the output of your network behaves under translations becomes particularly important when your images are too large to be fed into your network as a whole. To solve this the input image is split into tiles and ideally their output should fit together seamlessly. Do you see how seamless stitching is equivalent to translational equivariance under shifts under your tile size and multiples thereof?</li>\n",
    "    <li>How does padding affect the translational equivariance of your network?</li>\n",
    "    <li>Can you think of ways to design an architecture to achieve translation invariance?</li>\n",
    "</ol>\n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"height:2px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfccc397",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train a U-Net!\n",
    "We will get more into the details of evaluating semantic segmentation models in the next exercise. For now, we will provide an example pipeline that will train a U-Net to classify each pixel in an image of cells as foreground or background. You should have seen training loops like this in the pre-course exercises, so there is no implementation task here.\n",
    "\n",
    "We will use Tensorboard to log our training runs, so there is code in the train loop to log aspects of the training to Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb61c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = NucleiDataset(\"nuclei_train_data\", transforms.RandomCrop(256))\n",
    "train_loader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc37a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function: torch.nn.Module = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5243e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply training for one epoch\n",
    "def train(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    epoch,\n",
    "    log_interval=100,\n",
    "    log_image_interval=20,\n",
    "    tb_logger=None,\n",
    "    device=None,\n",
    "    early_stop=False,\n",
    "):\n",
    "    if device is None:\n",
    "        # You can pass in a device or we will default to using\n",
    "        # the gpu. Feel free to try training on the cpu to see\n",
    "        # what sort of performance difference there is\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "    # set the model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # iterate over the batches of this epoch\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # zero the gradients for this iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # apply model and calculate loss\n",
    "        prediction = model(x)\n",
    "        # if necessary, crop the masks to match the model output shape\n",
    "        if prediction.shape != y.shape:\n",
    "            y = center_crop(y, prediction.size()[2:])\n",
    "        if y.dtype != prediction.dtype:\n",
    "            y = y.type(prediction.dtype)\n",
    "        loss = loss_function(prediction, y)\n",
    "\n",
    "        # backpropagate the loss and adjust the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log to console\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_id * len(x),\n",
    "                    len(loader.dataset),\n",
    "                    100.0 * batch_id / len(loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # log to tensorboard\n",
    "        if tb_logger is not None:\n",
    "            step = epoch * len(loader) + batch_id\n",
    "            tb_logger.add_scalar(\n",
    "                tag=\"train_loss\", scalar_value=loss.item(), global_step=step\n",
    "            )\n",
    "            # check if we log images in this iteration\n",
    "            if step % log_image_interval == 0:\n",
    "                x = unnormalize(x)\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"input\", img_tensor=x.to(\"cpu\"), global_step=step\n",
    "                )\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"target\", img_tensor=y.to(\"cpu\"), global_step=step\n",
    "                )\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"prediction\",\n",
    "                    img_tensor=prediction.to(\"cpu\").detach(),\n",
    "                    global_step=step,\n",
    "                )\n",
    "                combined_image = torch.cat(\n",
    "                    [x, pad_to_size(y, x.size()), pad_to_size(prediction, x.size())],\n",
    "                    dim=3,\n",
    "                )\n",
    "\n",
    "                tb_logger.add_images(\n",
    "                    tag=\"input_target_prediction\",\n",
    "                    img_tensor=combined_image,\n",
    "                    global_step=step,\n",
    "                )\n",
    "\n",
    "        if early_stop and batch_id > 5:\n",
    "            print(\"Stopping test early!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973e020",
   "metadata": {},
   "source": [
    "The next two cells start tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cea86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to find an available port\n",
    "def find_free_port():\n",
    "    import socket\n",
    "\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "\n",
    "# Launch TensorBoard on the browser\n",
    "def launch_tensorboard(log_dir):\n",
    "    port = find_free_port()\n",
    "    tensorboard_cmd = f\"tensorboard --logdir={log_dir} --port={port}\"\n",
    "    process = subprocess.Popen(tensorboard_cmd, shell=True)\n",
    "    print(\n",
    "        f\"TensorBoard started at http://localhost:{port}. \\n\"\n",
    "        \"If you are using VSCode remote session, forward the port using the PORTS tab next to TERMINAL.\"\n",
    "    )\n",
    "    return process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60aa83d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Launch tensorboard and click on the link to view the logs.\n",
    "tensorboard_process = launch_tensorboard(\"unet_runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b753a90",
   "metadata": {
    "cell_marker": "\"\"\"",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "If you are using VSCode and a remote server, you will need to forward the port to view the tensorboard. <br>\n",
    "Take note of the port number was assigned in the previous cell.(i.e <code style=\"color: black\"> http://localhost:{port_number_assigned}</code>) <br>\n",
    "\n",
    "Locate the your VSCode terminal and select the <code style=\"color: black\">Ports</code> tab <br>\n",
    "<ul>\n",
    "<li>Add a new port with the <code style=\"color: black\">port_number_assigned</code>\n",
    "</ul>\n",
    "Click on the link to view the tensorboard and it should open in your browser.\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "If you launched jupyter lab from ssh terminal, add <code style=\"color: black\">--host &lt;your-server-name&gt;</code> to the tensorboard command below. <code style=\"color: black\">&lt;your-server-name&gt;</code> is the address of your compute node that ends in amazonaws.com.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531c32c",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3>Task 8: Declare and train a U-Net</h3>\n",
    "    <ol>\n",
    "        <li>Declare a U-Net with your favorite parameters!</li>\n",
    "        <li>Train the U-Net for 5 epochs, and inspect the training loss and the output predictions in Tensorboard.</li>\n",
    "        </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b583037",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "model = ...  # TASK 8.1: Declare your U-Net here and name it below\n",
    "model_name = \"my_fav_unet\"  # This name will be used in the tensorboard logs\n",
    "logger = SummaryWriter(f\"unet_runs/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1853bdb7",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# train for $5$ epochs\n",
    "# TASK 8.2: Run this cell and examine outputs in Tensorboard above!\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    train(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_function,\n",
    "        epoch=epoch,\n",
    "        tb_logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da134b74",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3>Task 9: Train more U-Nets! </h3>\n",
    "    <ol>\n",
    "        <li>Declare a new U-Net with different parameters. If you use a different name in the Tensorboard logger, you will be able to know which logs are from which run.</li>\n",
    "        <li>Train your new U-Net - does it perform better or worse? How can you tell?</li>\n",
    "        <li>Keep tinkering with different architectures until the end of the exercise!</li>\n",
    "        </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1173523",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Checkpoint 5</h2>\n",
    "\n",
    "Congratulations! You trained your first UNet that you implemented all by yourself!\n",
    "\n",
    "We will keep using this U-Net throughout the rest of the exercises. Whenever you see an import like `import dlmbl-unet` or\n",
    "`from dlmbl-unet import UNet` it will be importing from [this repository](https://github.com/dlmbl/dlmbl-unet) which contains the solution to this notebook as a package (including the bonus exercises so don't peek just yet if you wanna solve the bonus too).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd06914",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus 1: 3D UNet\n",
    "The UNet you implemented so far only works for 2D images, but in microscopy we often have 3D data that also needs to be processed as such, i.e. for some tasks it is important that the network's receptive field is 3D. So in this bonus exercise we will change our implementation to make the number of dimensions configurable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63d735",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3>Task 10: Make U-Net building blocks configurable for 2D and 3D data! </h3>\n",
    "    To make the same class usable for 2D and 3D data we will add an argument `ndim` to each building block.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc96391",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class Downsample(torch.nn.Module):\n",
    "    def __init__(self, downsample_factor: int, ndim: int = 2):\n",
    "        \"\"\"Initialize a MaxPool2d module with the input downsample fator\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if ndim not in (2, 3):\n",
    "            msg = f\"Invalid number of dimensions: {ndim=}. Options are 2 or 3.\"\n",
    "            raise ValueError(msg)\n",
    "        self.ndim = ndim\n",
    "        self.downsample_factor = downsample_factor\n",
    "        # TASK 10A: Initialize the maxpool module\n",
    "        # Define what the downop should be based on `ndim`.\n",
    "        self.down = ...  # YOUR CODE HERE\n",
    "\n",
    "    def check_valid(self, image_size: tuple[int, ...]) -> bool:\n",
    "        \"\"\"Check if the downsample factor evenly divides each image dimension.\n",
    "        Returns `True` for valid image sizes and `False` for invalid image sizes.\n",
    "        Note: there are multiple ways to do this!\n",
    "        \"\"\"\n",
    "        # TASK 10B: Check that the image_size is valid to use with the downsample factor\n",
    "        # YOUR CODE HERE\n",
    "        # You can likely copy this from Task 2B2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.check_valid(tuple(x.size()[-self.ndim:])):\n",
    "            raise RuntimeError(\n",
    "                \"Can not downsample shape %s with factor %s\"\n",
    "                % (x.size(), self.downsample_factor)\n",
    "            )\n",
    "\n",
    "        return self.down(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4434ebd",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        padding: str = \"same\",\n",
    "        ndim: int = 2,\n",
    "    ):\n",
    "        \"\"\"A convolution block for a U-Net. Contains two convolutions, each followed by a ReLU.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of input channels for this conv block. Depends on\n",
    "                the layer and side of the U-Net and the hyperparameters.\n",
    "            out_channels (int): The number of output channels for this conv block. Depends on\n",
    "                the layer and side of the U-Net and the hyperparameters.\n",
    "            kernel_size (int): The size of the kernel. A kernel size of N signifies an\n",
    "                NxN square kernel.\n",
    "            padding (str): The type of convolution padding to use. Either \"same\" or \"valid\".\n",
    "                Defaults to \"same\".\n",
    "            ndim (int): Number of dimensions for the convolution operation. Use 2 for 2D\n",
    "                convolutions and 3 for 3D convolutions. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if ndim not in (2, 3):\n",
    "            msg = f\"Invalid number of dimensions: {ndim=}. Options are 2 or 3.\"\n",
    "            raise ValueError(msg)\n",
    "        if kernel_size % 2 == 0:\n",
    "            msg = \"Only allowing odd kernel sizes.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        # TASK 10C: Initialize your modules and define layers.\n",
    "        # Use the convolution module matching `ndim`.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        for _name, layer in self.named_modules():\n",
    "            if isinstance(layer, (torch.nn.Conv2d, torch.nn.Conv3d)):\n",
    "                torch.nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TASK 10D: Apply the modules you defined to the input x\n",
    "        ...  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02672db3",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class OutputConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        activation: torch.nn.Module | None = None,\n",
    "        ndim: int = 2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A module that uses a convolution with kernel size 1 to get the appropriate\n",
    "        number of output channels, and then optionally applies a final activation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): The number of feature maps that will be input to the\n",
    "                OutputConv block.\n",
    "            out_channels (int): The number of channels that you want in the output\n",
    "            activation (str | None, optional): Accepts the name of any torch activation\n",
    "                function  (e.g., ``ReLU`` for ``torch.nn.ReLU``) or None for no final\n",
    "                activation. Defaults to None.\n",
    "            ndim (int): Number of dimensions for convolution operation. Use 2 for 2D\n",
    "                convolutions and 3 for 3D convolutions. Defaults to 2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if ndim not in (2, 3):\n",
    "            msg = f\"Invalid number of dimensions: {ndim=}. Options are 2 or 3.\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # TASK 10E: Define the convolution submodule.\n",
    "        # Use the convolution module matching `ndim`.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TASK 10F: Implement the forward function\n",
    "        # YOUR CODE HERE\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc69521",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int = 1,\n",
    "        final_activation: torch.nn.Module | None = None,\n",
    "        num_fmaps: int = 64,\n",
    "        fmap_inc_factor: int = 2,\n",
    "        downsample_factor: int = 2,\n",
    "        kernel_size: int = 3,\n",
    "        padding: str = \"same\",\n",
    "        upsample_mode: str = \"nearest\",\n",
    "        ndim: int = 2,\n",
    "    ):\n",
    "        \"\"\"A U-Net for 2D or 3D input that expects tensors shaped like:\n",
    "            ``(batch, channels, height, width)`` or ``(batch, channels, depth, height, width)``,\n",
    "            respectively.\n",
    "\n",
    "        Args:\n",
    "            depth:\n",
    "                The number of levels in the U-Net. 2 is the smallest that really\n",
    "                makes sense for the U-Net architecture, as a one layer U-Net is\n",
    "                basically just 2 conv blocks.\n",
    "            in_channels:\n",
    "                The number of input channels in your dataset.\n",
    "            out_channels (optional):\n",
    "                How many output channels you want. Depends on your task. Defaults to 1.\n",
    "            final_activation (optional):\n",
    "                What activation to use in your final output block. Depends on your task.\n",
    "                Defaults to None.\n",
    "            num_fmaps (optional):\n",
    "                The number of feature maps in the first layer. Defaults to 64.\n",
    "            fmap_inc_factor (optional):\n",
    "                By how much to multiply the number of feature maps between\n",
    "                layers. Encoder layer ``l`` will have ``num_fmaps*fmap_inc_factor**l``\n",
    "                output feature maps. Defaults to 2.\n",
    "            downsample_factor (optional):\n",
    "                Factor to use for down- and up-sampling the feature maps between layers.\n",
    "                Defaults to 2.\n",
    "            kernel_size (int, optional):\n",
    "                Kernel size to use in convolutions on both sides of the UNet.\n",
    "                Defaults to 3.\n",
    "            padding (str, optional):\n",
    "                How to pad convolutions. Either 'same' or 'valid'. Defaults to \"same.\"\n",
    "            upsample_mode (str, optional):\n",
    "                The upsampling mode to pass to torch.nn.Upsample. Usually \"nearest\"\n",
    "                or \"bilinear.\" Defaults to \"nearest.\"\n",
    "            ndim (int, optional): Number of dimensions for the U-Net. Use 2 for 2D U-Net and\n",
    "                3 for 3D U-Net. Defaults to 2.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        if ndim not in (2, 3):\n",
    "            msg = f\"Invalid number of dimensions: {ndim=}. Options are 2 or 3.\"\n",
    "            raise ValueError(msg)\n",
    "        self.depth = depth\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.final_activation = final_activation\n",
    "        self.num_fmaps = num_fmaps\n",
    "        self.fmap_inc_factor = fmap_inc_factor\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.upsample_mode = upsample_mode\n",
    "\n",
    "        # left convolutional passes\n",
    "        self.left_convs = torch.nn.ModuleList()\n",
    "        # TASK 10G: Initialize list here\n",
    "        # Loop through each level of the encoder from top (level=0) to bottom (level=self.depth - 1)\n",
    "        for level in range(self.depth): \n",
    "            # conv = \n",
    "            # Adding conv module to the list\n",
    "            self.left_convs.append(conv)\n",
    "\n",
    "        # right convolutional passes\n",
    "        self.right_convs = torch.nn.ModuleList()\n",
    "        # TASK 10H: Initialize list here\n",
    "        # Loop through each level of the decoder from top (level=0) to one above bottom (level=self.depth - 2)\n",
    "        for level in range(self.depth - 1):\n",
    "            # Initialize conv module\n",
    "            # conv = \n",
    "            # Adding conv module to the list\n",
    "            self.right_convs.append(conv)\n",
    "\n",
    "        # TASK 10I: Initialize other modules here\n",
    "        # Same here, copy over from TASK 6.2, but make sure to add the ndim argument\n",
    "        # as needed.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # left side\n",
    "        # Hint - you will need the outputs of each convolutional block in the encoder for the skip connection, so you need to hold on to those output tensors\n",
    "        for i in range(self.depth - 1):\n",
    "            # TASK 10L: Implement encoder here\n",
    "            # Copy from TASK 6.3A\n",
    "            ...\n",
    "\n",
    "        # bottom\n",
    "        # TASK 10M: Implement bottom of U-Net here\n",
    "        # Copy from TASK 6.3B\n",
    "\n",
    "        # right\n",
    "        for i in range(0, self.depth - 1)[::-1]:\n",
    "            # TASK 10N: Implement decoder here\n",
    "            # Copy from TASK 6.3C\n",
    "            ...\n",
    "        # TASK 10O: Apply the final convolution and return the output\n",
    "        # Copy from TASK 6.3D\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247815c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Run the 3d test for your implementation of the UNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c95340",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestUNet(UNet).run3d()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccec16e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bonus 2: Tilable UNet\n",
    "In order to process arbitrarily large images, we can divide the image into smaller tiles and process each tile separately. To avoid artifacts at tile boundaries, the U-Net needs to be equivariant w.r.t. translations by the tile size. Add a cropping layer at the end of the U-Net that crops the output to a size for which the U-Net is equivariant. Can tranlsational equivariance be achieved for any parameter setting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c52b3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <h3>Task 11: Implement a Tilable U-Net</h3>\n",
    "    <p>Modify the U-Net to make it suitable for processing large images by tiling. You'll need to add a cropping\n",
    "         mechanism to ensure translational equivariance.</p>\n",
    "    <ol>\n",
    "        <li>Copy over your UNet implemntation from Task 10.</li>\n",
    "        <li>Crop the output of the U-Net to an appropriate tile size</li>\n",
    "        <li>Add a conditional to only crop the outputs for U-Nets that can be translation equivariant.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523bcb06",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        depth: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int = 1,\n",
    "        final_activation: torch.nn.Module | None = None,\n",
    "        num_fmaps: int = 64,\n",
    "        fmap_inc_factor: int = 2,\n",
    "        downsample_factor: int = 2,\n",
    "        kernel_size: int = 3,\n",
    "        padding: str = \"same\",\n",
    "        upsample_mode: str = \"nearest\",\n",
    "        ndim: int = 2,\n",
    "    ):\n",
    "        \"\"\"A U-Net for 2D or 3D input that expects tensors shaped like:\n",
    "          ``(batch, channels, height, width)`` or ``(batch, channels, depth, height, width)``,\n",
    "          respectively.\n",
    "      Args:\n",
    "          depth:\n",
    "              The number of levels in the U-Net. 2 is the smallest that really\n",
    "              makes sense for the U-Net architecture, as a one layer U-Net is\n",
    "              basically just 2 conv blocks.\n",
    "          in_channels:\n",
    "              The number of input channels in your dataset.\n",
    "          out_channels (optional):\n",
    "              How many output channels you want. Depends on your task. Defaults to 1.\n",
    "          final_activation (optional):\n",
    "              What activation to use in your final output block. Depends on your task.\n",
    "              Defaults to None.\n",
    "          num_fmaps (optional):\n",
    "              The number of feature maps in the first layer. Defaults to 64.\n",
    "          fmap_inc_factor (optional):\n",
    "              By how much to multiply the number of feature maps between\n",
    "              layers. Encoder layer ``l`` will have ``num_fmaps*fmap_inc_factor**l``\n",
    "              output feature maps. Defaults to 2.\n",
    "          downsample_factor (optional):\n",
    "              Factor to use for down- and up-sampling the feature maps between layers.\n",
    "              Defaults to 2.\n",
    "          kernel_size (int, optional):\n",
    "              Kernel size to use in convolutions on both sides of the UNet.\n",
    "              Defaults to 3.\n",
    "          padding (str, optional):\n",
    "              How to pad convolutions. Either 'same' or 'valid'. Defaults to \"same.\"\n",
    "          upsample_mode (str, optional):\n",
    "              The upsampling mode to pass to torch.nn.Upsample. Usually \"nearest\"\n",
    "              or \"bilinear.\" Defaults to \"nearest.\"\n",
    "          ndim (int, optional): Number of dimensions for the U-Net. Use 2 for 2D U-Net and\n",
    "              3 for 3D U-Net. Defaults to 2.\n",
    "      \"\"\"\n",
    "        super().__init__()\n",
    "        # TASK 11.1: Initialize the modules of the U-Net\n",
    "        # Copy from TASK 10\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TASK 11.1: Apply the modules of the U-Net\n",
    "        # Copy from TASK 10\n",
    "        \n",
    "        # TASK 11.3: Add conditional \n",
    "        \n",
    "        # TASK 11.2: Crop the output of the U-Net to an appropriate tile size\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb8880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unet_tests.TestUNet(UNet).run3d_tiled()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "main_language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
